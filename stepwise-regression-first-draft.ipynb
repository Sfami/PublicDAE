{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **I will further manipulate this code using the data Mo created, I was supposed to now do it on the original data I had but it would be counterproductive. This draft was based on a simple dataset I created just to see that the code works as intended.**","metadata":{}},{"cell_type":"markdown","source":"# <center> Model Fitting </center>\nFor the purpose of this study, we fit the data to OLS regressions. We first explain what these models are and subsequently elaborate how we use them.<br> \n\nMultiple linear regression<br>\nThis kind of regression model is used to analyse a linear relationship between response variable $Y$ and covariates $X_i$. Multiple regression model considers two or more explanatory variables:\n$$ y_i = \\beta_0 + \\beta_1x_{1,i} + \\beta_2x_{2,i}+ \\dots + \\beta_mx_{m,i} + \\epsilon_i $$ \nThe coefficients $\\beta_{1},\\dots,\\beta_{k}$ measure the effect of each individual predictor after taking into account the effects of all the other predictors. The error term $\\epsilon_i$ reflects random variation in $Y$ from variables other than $x$. \nWith this regression modeling, estimate the regression function:\n$$ \\hat{r}(x) = \\hat{\\beta_0} + \\hat{\\beta_1}x $$\n\nCategorical Predictors<br>\nA regression that includes a categorical predictor with $n$ categories has the equation:\n$$ y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_{I1} I_{1,i} + \\beta_{I2} I_{2,i} + \\dots + \\beta_{I(n-1)} I_{n-1,i} \\epsilon_i $$\nwhere $x_{1,i}$ and $y_i$ are continuous variables and $I_{2,i}$ is an indicator variable.\nThe model is interpreted as a linear regression model that has an intercept that depends on the category of the individual.\n\nAssumptions:<br>\nerror term $\\epsilon_i$ is assumed to be a normal random variable<br>\n$r(x)$ is linear. <br>\n\nWe use data on household monthly income, salaries_1 and salaries_2 (these are encoded variables which tells us the if the household main source of income is salaries or not) to fit the multiple linear regression. To analyse the relationship between these variables we come across issues such an extremly high condition number indicating that there is multicollinearity in with the predictors. We also have a low adjusted R squared.","metadata":{"execution":{"iopub.status.busy":"2021-06-03T20:23:27.223954Z","iopub.execute_input":"2021-06-03T20:23:27.224435Z","iopub.status.idle":"2021-06-03T20:23:27.239879Z","shell.execute_reply.started":"2021-06-03T20:23:27.224373Z","shell.execute_reply":"2021-06-03T20:23:27.238706Z"}}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"fit1 = smf.ols(formula=\"mon_income ~ salaries_1+salaries_2\", data=full_data_dummies).fit(fit_intercept=False)\n\nprint(fit1.summary())\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"without_response = full_data_dummies.drop(['mon_income','year'],axis=1)\nresponse='mon_income'\nformula = \"{} ~ {} + 1\".format(response,\n                                   ' + '.join(without_response.columns))\n\nfit3 = smf.ols(formula, data = full_data_dummies).fit(fit_intercept=False)\nprint(fit3.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(full_data_dummies, test_size=0.2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n## Variable and Model Selection\n\nFrom our given dataset, there are a large number of variables to choose from. As we have seen from the above models, including all variables we run into multicollinearity. Including a restricted number we may have subjective models. Thus the choice of which model to include is dependent on the predictive ability of the entire model.\n Common measures to assess the predictive ability of the model are:\n* AIC, AICc, BIC \n* $R^2$ and adj-$R^2$ \n\n\n#### Stepwise Regression\nSince we have a large number of potential predictors, there are a lot of subsets to go through. Thus, we use stepwise regression procedures. These procedures find a good model, but it may not be the best model.\n\n1) Check for multicollinearity:\nThis is when the predictores are linearly dependent, thus at least one predictor is redundatant. This causes problems as we have unstable samples. We employ Variance Inflation Factors(VIF) to check for multicollinearity.Interpretation of the VIF is  variance of coefficient estimator is inflated by a factor of VIF in comparison to if the variable was not correlated. We will drop thenredundant feature\n\n2) Backward Regression: \nstart with all variables and measure the p value, then remove at a variable time and measure p value. Then choose model with best pvalue and iterate through the last two steps \n\n3) Forward Regression\nstart with model that has only the intercept then include one variable at a time and measure adjusted $R^2$. Choose model with best $R^2$ and iterate through the last two steps","metadata":{}},{"cell_type":"code","source":"def multicol(x, thresh=10):\n    x = x.drop(['mon_income'],axis=1)\n    variables=list(range(x.shape[1]))\n    dropped= True\n    while dropped:\n        dropped=False\n        vif = [variance_inflation_factor(x.iloc[:,variables].values, ix) for ix in variables]\n        maxloc = vif.index(max(vif))\n        if max(vif) > thresh:\n            print('dropping \\'' + x.iloc[:,variables].columns[maxloc] + '\\' at index: ' + str(maxloc))\n            x.drop(x.columns[variables[maxloc]],1, inplace=True)\n            variables = list(range(x.shape[1]))\n            dropped=True  \n    t = x.columns[variables]\n    return t","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def multicollinearity(x, sample_size, thresh=10):\n    sample_train=resample(x,n_samples=sample_size)\n    remain=multicol(sample_train)\n    return remain","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remaining=multicollinearity(train, sample_size=1000, thresh=5)\nremaining","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train[remaining]\ny = train['mon_income']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def backward_selction(x,y):\n    cols= list(X.columns)\n    pmax = 1\n    verbose=True\n    while (len(cols)>0):\n        p=[]\n        x_1=pd.DataFrame(X[cols])\n        x_1= sm.add_constant(x_1)\n        model= sm.OLS(y,x_1).fit()\n        p= pd.Series(model.pvalues.values[1:],index=cols)\n        pmax= max(p)\n        feature_with_p_max=p.idxmax()\n        if(pmax>0.05):\n            cols.remove(feature_with_p_max)\n            print('Drop {:30} with p-value {:.6}'.format(feature_with_p_max, pmax))\n        else:\n            break\n    selected_features= cols\n    return selected_features\nselected_features=backward_selction(X,y)\nselected_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss=train[selected_features]\nselected_data= ss.merge(y.to_frame(), left_index=True, right_index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statsmodels.formula.api as smf\n\ndef forward_selected(data):\n    \n    response='mon_income'\n    remaining = set(data.columns)\n    remaining.remove(response)\n    selected = []\n    current_r= 0.0\n    best_r = 0.0\n    count=0\n    \n    while remaining and current_r == best_r:\n        scores_with_candidates = []\n        for variable in remaining:\n             \n            formula = \"{} ~ {} + 1\".format(response,' + '.join(selected + [variable]))\n            score = smf.ols(formula, data).fit(fit_intercept=False).rsquared_adj\n            scores_with_candidates.append((score, variable))\n        count+=1\n        scores_with_candidates.sort()\n        best_r, best_candidate = scores_with_candidates.pop()\n        if ((current_r < best_r) or (count>=20)):\n            print('Adding: {}'.format(best_candidate))\n            count=0\n            remaining.remove(best_candidate)\n            selected.append(best_candidate)\n            current_r = best_r\n    formula = \"{} ~ {} + 1\".format(response,' + '.join(selected))\n    model = smf.ols(formula, data).fit(fit_intercept=False)\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=forward_selected(selected_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking if the remaining predictors contribute to the overall model. We employ adjusted R square for this and we see that all the predictors are included in the optial model. That is if they are all jointly significant. As we have mentioned that while these procedures produces a good model but it might not be the optimal one. Thus we proceed to coduct bootstrapping of those steps to furture refine the chosen model and features.","metadata":{}},{"cell_type":"code","source":"def bootstrapping(data,simulations, sample_size):\n    optimal_models=[]\n    \n    for i in range(0,simulations):\n        \n        print('Starting iteration: {:}'.format(i))\n        \n        data_sampled=resample(data,n_samples=sample_size)\n        remaining=multicollinearity(data_sampled, sample_size=sample_size, thresh=5)\n    \n        \n        X = train[remaining]\n        y = train['mon_income']\n\n        result = backward_selction(X, y) \n        result.append('mon_income')\n        \n        model=forward_selected(data_sampled[result])\n        choosen=[]\n        choosen=[model,model.params,model.bse,model.pvalues,model.conf_int(),model.condition_number,model.rsquared_adj]\n        optimal_models.append(choosen)\n        print('Finish iteration: {:}'.format(i))\n    return optimal_models\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_dataa=bootstrapping(train,simulations=300, sample_size=5000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def common_features(selected_dataa,adj_r,iters_fraction):\n    features = {}\n    for i in range(len(selected_dataa)):\n        if (selected_dataa[i][6]>adj_r):\n            feat=list(selected_dataa[i][1].index)\n            for f in feat:\n                try:\n                    features[f]+=1\n                except:\n                    features[f]=1\n    valid=[]\n    for feat,value in features.items():\n        if ((value>=iters_fraction) and (feat!='Intercept')):\n            valid.append(feat)\n    return valid","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_data1=common_features(selected_dataa,0.4,250)\nselected_data1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### Residual Analysis\n\nNow that we have selected the optimal model, we analyse the residuals and check if we met the assumption of the model. \nWe perform the following four plots to check for the following:<br>\n1) heteroskedasticity<br>\n2) if residuals are related to predictors<br>\n3)  if residuals are related to dependent variable<br>\n\nIf these assuptions are not satisfied the the least square estimate is not efficient. The estimated will be biased at the statistical tests will be misleading.\n","metadata":{}},{"cell_type":"code","source":"import matplotlib as mpl\nmpl.rcParams['agg.path.chunksize']=10000\n\n# figure window\nfig = plt.figure(figsize=[16,16]);\n    # subplots\nax1 = plt.subplot2grid((2,2), (0,0), colspan=2)\n\n\nresponse='mon_income'\nformula = \"{} ~ {} + 1\".format(response,\n                                   ' + '.join(selected_data1))\nfit3 = smf.ols(formula, data=train).fit(fit_intercept=False)\n#residual plot\nfit3.resid.plot(ax=ax1)\nplt.suptitle('Residuals from Linear regression model', size = 20);\nplt.subplots_adjust(top=0.9)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import month_plot, plot_acf, plot_pacf\n# figure window\nfig = plt.figure(figsize=[10,6]);\nax1 = plt.subplot2grid((2,2), (0,0), colspan=2)\n# fit regression model\nresponse='mon_income'\nformula = \"{} ~ {} + 1\".format(response,\n                                   ' + '.join(selected_data1))\nfit3 = smf.ols(formula, data=train).fit(fit_intercept=False)\n# ACF\nplot_acf(fit3.resid, ax=ax1, lags=50, zero=False)    \nplt.subplots_adjust(top=0.9)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# figure window\nfig = plt.figure(figsize=[10,6]);\n\n# fit regression model\nresponse='mon_income'\nformula = \"{} ~ {} + 1\".format(response,\n                                   ' + '.join(selected_data1))\nfit3 = smf.ols(formula, data=train).fit(fit_intercept=False)\n\n# residual histogram\nsns.distplot(fit3.resid, kde=True, rug=True, hist = True)\n# tight format\nfig.tight_layout()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = plt.scatter(y = fit3.resid, x = fit3.predict())\nplt.xlabel('Fitted');\nplt.ylabel('Residuals');\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response='mon_income'\ntest_data=test\n\nformula = \"{} ~ {} + 1\".format(response,\n                                   ' + '.join(selected_data1))\nmodel = smf.ols(formula, test_data).fit()\nprint(model.summary())\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}